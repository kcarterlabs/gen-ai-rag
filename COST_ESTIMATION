# COST_ESTIMATION.md – Serverless RAG Cost Modeling

This document estimates infrastructure and model usage costs
for the Serverless RAG system.

Assumptions:
- Bedrock Embedding Model
- Bedrock Chat Model
- S3 vector storage
- DynamoDB cost tracking
- Lambda + API Gateway

All costs are approximate and should be verified against current AWS pricing.

---

# 1. Pricing Inputs (Example Numbers)

Replace with current AWS pricing when deploying.

Embedding model:
$0.0001 per 1,000 tokens

Chat model:
$0.002 per 1,000 input tokens
$0.002 per 1,000 output tokens

Lambda:
$0.20 per 1M requests
$0.00001667 per GB-second

S3:
$0.023 per GB-month (standard)

DynamoDB:
On-demand: ~$1.25 per million writes

---

# 2. Ingestion Cost (Per Document)

Example:
- 10,000 words document
- ~40,000 characters
- ~10,000 tokens

Embedding cost:
10,000 tokens ÷ 1,000 × $0.0001
= $0.001 per document

If 1,000 documents ingested:
$1 total embedding cost

S3 storage:
Assume each doc produces:
- 20 chunks
- 1,536-dimension vectors
- ~10KB per chunk JSON

20 chunks × 10KB = 200KB per document

1,000 documents = 200MB
S3 cost:
0.2 GB × $0.023 = ~$0.0046/month

Storage cost is negligible compared to model usage.

---

# 3. Per Query Cost

Assumptions:
- Query = 50 tokens
- Retrieved context = 1,500 tokens
- Model output = 500 tokens

Total input tokens:
1,550 tokens

Input cost:
1,550 ÷ 1,000 × $0.002 = $0.0031

Output cost:
500 ÷ 1,000 × $0.002 = $0.001

Total LLM cost per query:
≈ $0.0041

Embedding query vector:
50 tokens ÷ 1,000 × $0.0001
= $0.000005

Total per query:
≈ $0.0041

Embedding cost is negligible per query.

---

# 4. Monthly Cost Scenarios

## Light Usage

- 1,000 queries/month
- 100 new documents

Query cost:
1,000 × $0.0041 = $4.10

Embedding cost:
100 × $0.001 = $0.10

Infra (Lambda + API + DynamoDB):
~$1–3

Total:
~$6–8/month

---

## Moderate Usage

- 10,000 queries/month
- 1,000 documents

Query cost:
10,000 × $0.0041 = $41

Embedding:
$1

Infra:
$5–10

Total:
~$50/month

---

## Heavy Usage

- 100,000 queries/month

Query cost:
100,000 × $0.0041 = $410

Infra:
$20–40

Total:
~$450/month

Primary cost driver: LLM tokens.

---

# 5. Cost Optimization Strategies

## Reduce Context Size
- Lower chunk size
- Lower top_k
- Use context compression

## Reduce Output Tokens
- Limit max_tokens_to_sample
- Lower verbosity

## Deduplicate Embeddings
- Hash chunks before embedding

## Adaptive Retrieval
- Short queries = smaller top_k
- Large queries = larger top_k

## Tier-Based Limits
- Free tier:
  - Lower token budget
  - Lower top_k
- Premium tier:
  - Higher limits

---

# 6. Cost Formula (Generalized)

Per Query Cost:

C = (InputTokens × InputPrice/1000)
  + (OutputTokens × OutputPrice/1000)

Monthly Cost:

Total = (QueryCost × QueriesPerMonth)
      + (EmbeddingCost × DocsPerMonth)
      + Infra

---

# 7. Architectural Trade-Offs

S3 Vector Store:
✔ Near-zero idle cost
✔ Extremely cheap storage
✖ Slower retrieval vs OpenSearch

OpenSearch:
✔ Faster similarity search
✔ Scales better for large corpora
✖ Constant instance cost ($70+/month minimum)

For small-to-medium systems:
S3 is significantly cheaper.

---

# 8. Key Exam Talking Points

- Primary cost driver = LLM tokens
- Chunk size directly impacts cost
- Retrieval depth (top_k) impacts cost
- Embedding cost is small compared to generation
- Storage cost is negligible
- Always implement token budget enforcement
- Use CloudWatch + DynamoDB for cost tracking

If you can explain this clearly, you are thinking at GenAI Architect level.
